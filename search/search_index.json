{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"bioflows Summary This is a guide to using the bioflows package for running standard pipelines to analyse NGS datasets. Currently, we have implemented some standard workflows alongwith tutorials using this package. To start using bioflows use the quick start to go to any of the tutorials implemented Quickstart RNA Seq with GSNAP RNA-Seq on CCV QIIME2 QIIME2 on CCV GATK GERMLINE Variant Calling on CCV GATK SOMATIC Somatic variant Calling on CCV Motivation A primary objective of the Computational Biology Core at Brown's Centre for Computational Biology of Human Disease , is to enable reproducibility in computational analysis of NGS data. Critical to this objective is to provide a simple tool for creating/running bioinformatics workflows as well as consistent software environments across multiple platforms. To this we: Developed bioflows a workflow tool to ensure consistency in analysis steps and stages with interoperability across multiple job submission systems Use CONDA package management system for managing software tools Container based approach using docker for cross-platform interoperability of the analysis environment Overview bioflows is an user-friendly python implementation of a workflow manager. The user is expected to not have any programming knowledge and needs to only provide a control file in a YAML format, chosen for its human readability. The goal here is to provide users with a simple and straight-forward interface for processing NGS datasets with many samples using standard bioinformatics pipelines, e.g RNA-seq, GATK variant calling etc. The tool is developed to alleviate some of the primary issues with scaling up pipelines, such as file naming, management of data, output and logs. How it works bioflows uses two main python packages: 1) luigi developed at spotify for managing dependencies among task and 2) SAGA python API to launch jobs across different types of systems. All the necessary tools are provided from the ** CBCs anaconda channel . Key Features Currently bioflows provides the following features: Simple management of data logs results scripts A module to easily download data from NCBI's SRA archive and to optionally directly continue processing the data through the pipelines. A few key elements of this module are: Only download and convert the data to usable fastqs Data from multiple runs are concatenated automatically Metadata associated with the SRA data is also provided as a seperate table Conda packages for all dependencies are already pre-built and provided alongwith the software Conda Package Management CONDA is a system agnostic software package management system based on the Anaconda python distribution to ensure that a software and all its dependencies are bundled together. These conda packages can be downloaded from various publicly available repositories called channels and one such channel for bio-informatics tools is bioconda. For ensuring reproducibily, we have established a publicly accessible channel for all programs that are included with wrappers within the bioflows tool through the compbiocore channel . In this channel, we have also provided conda packages of all software used including the bioflows package itself. To download specific packages or the bioflows tool use the following command into your conda environment: Note bioflows and its dependencies are available as conda packages for the linux OS only. To use bioflows in other OSes you will need to use the docker container approach More detailed instructions on how to install anaconda and use the conda environments can be found in the anaconda documentation for: Installation Getting started","title":"Home"},{"location":"#bioflows","text":"","title":"bioflows"},{"location":"#summary","text":"This is a guide to using the bioflows package for running standard pipelines to analyse NGS datasets. Currently, we have implemented some standard workflows alongwith tutorials using this package. To start using bioflows use the quick start to go to any of the tutorials implemented","title":"Summary"},{"location":"#quickstart","text":"RNA Seq with GSNAP RNA-Seq on CCV QIIME2 QIIME2 on CCV GATK GERMLINE Variant Calling on CCV GATK SOMATIC Somatic variant Calling on CCV","title":"Quickstart"},{"location":"#motivation","text":"A primary objective of the Computational Biology Core at Brown's Centre for Computational Biology of Human Disease , is to enable reproducibility in computational analysis of NGS data. Critical to this objective is to provide a simple tool for creating/running bioinformatics workflows as well as consistent software environments across multiple platforms. To this we: Developed bioflows a workflow tool to ensure consistency in analysis steps and stages with interoperability across multiple job submission systems Use CONDA package management system for managing software tools Container based approach using docker for cross-platform interoperability of the analysis environment","title":"Motivation"},{"location":"#overview","text":"bioflows is an user-friendly python implementation of a workflow manager. The user is expected to not have any programming knowledge and needs to only provide a control file in a YAML format, chosen for its human readability. The goal here is to provide users with a simple and straight-forward interface for processing NGS datasets with many samples using standard bioinformatics pipelines, e.g RNA-seq, GATK variant calling etc. The tool is developed to alleviate some of the primary issues with scaling up pipelines, such as file naming, management of data, output and logs.","title":"Overview"},{"location":"#how-it-works","text":"bioflows uses two main python packages: 1) luigi developed at spotify for managing dependencies among task and 2) SAGA python API to launch jobs across different types of systems. All the necessary tools are provided from the ** CBCs anaconda channel .","title":"How it works"},{"location":"#key-features","text":"Currently bioflows provides the following features: Simple management of data logs results scripts A module to easily download data from NCBI's SRA archive and to optionally directly continue processing the data through the pipelines. A few key elements of this module are: Only download and convert the data to usable fastqs Data from multiple runs are concatenated automatically Metadata associated with the SRA data is also provided as a seperate table Conda packages for all dependencies are already pre-built and provided alongwith the software","title":"Key Features"},{"location":"#conda-package-management","text":"CONDA is a system agnostic software package management system based on the Anaconda python distribution to ensure that a software and all its dependencies are bundled together. These conda packages can be downloaded from various publicly available repositories called channels and one such channel for bio-informatics tools is bioconda. For ensuring reproducibily, we have established a publicly accessible channel for all programs that are included with wrappers within the bioflows tool through the compbiocore channel . In this channel, we have also provided conda packages of all software used including the bioflows package itself. To download specific packages or the bioflows tool use the following command into your conda environment: Note bioflows and its dependencies are available as conda packages for the linux OS only. To use bioflows in other OSes you will need to use the docker container approach More detailed instructions on how to install anaconda and use the conda environments can be found in the anaconda documentation for: Installation Getting started","title":"Conda Package Management"},{"location":"intro/","text":"Using bioflows Introduction This is a guide to using the bioflows package for standard workflows to analyse NGS datasets. Currently, we have implemented one standard RNA-seq workflow and a tutorial is included for standardized processing of RNA-seq data using bioflows . Overview of bioflows","title":"Docs"},{"location":"intro/#using-bioflows","text":"","title":"Using bioflows"},{"location":"intro/#introduction","text":"This is a guide to using the bioflows package for standard workflows to analyse NGS datasets. Currently, we have implemented one standard RNA-seq workflow and a tutorial is included for standardized processing of RNA-seq data using bioflows .","title":"Introduction"},{"location":"intro/#overview-of-bioflows","text":"","title":"Overview of bioflows"},{"location":"yaml_description/","text":"bioflows YAML control file Specifications Project information section bioproject : This will be an unique identifier for your project. This is adopted from the NCBI SRA format structure, so if you use an SRA dataset you can employ these ids experiment : An identifier for your experiment such as RNA-seq, ChIP-seq etc sample_manifest : This section contains two sections fastq_file : The full path to the sample to fastq map file. This file is in a three column comma separated format with each line formatted as: sample_id, path_to_fastq_file_for_read1, path_to_fastq_file_for_read2 The sample_id is unique and if you are using single end data you just need to specify one column as shown below: sample_id, path_to_fastq_file metadata : This is all the metadata associated with a given sample_id if available such as gender, extraction date etc. This should also be a CSV format file. Currently, not necessary as this information is not yet used Global run parameters run_parms : This section specifies the global parameters for the current analysis conda_command : This is the command used to activate your conda environment work_dir : The working directory for analysis usually created on /gpfs/scratch log_dir : The subdirectory for all the log files paired_end : Whether data consists of paired end reads or single end reads (True/False) Warning This is an experimental feature and may not work as intended ssh_user : The user name if workflow is run from a local machine saga_scheduler : The scheduler being used, for CCV the value used here is slurm. !!! note Currently only tested with slurm scheduler. Will add test to others soon gtf_file : The full path to the gtf file for gene annotations, needed if you are planning to run rna-seq analysis Workflow parameters workflow_sequence : This section specifies the sequence of tools to be used and the options passed to tools as well as the job parameters if using a scheduler such as slurm fastqc :If you want to use the default parameters use default else you can use any of the options provided by the program. See the example for GSNAP below on how to do that. See the documentation for the options for fastqc. gsnap : Here we give an example of two sections as we need to pass the index information to the aligner options : Specify program options here. In this example we specify the following -d: The genome index for GSNAP -s: and the splicesites file location for GSNAP. The format is exactly that as to what you would specify on the command line for the program -d Ensembl_mus_GRCm38.p5_rel89 -s Mus_musculus.GRCm38.89.splicesites.iit See the documentation for the GSNAP program for more options job_param s: This section specifies parameters for job submission such as memory, number of cores etc ncpus: 16 mem: 40000 time: 60 qualimap_rnaseq : Run the qualimap module for RNAseq with the default settings The final YAML control file should look as below to run a test example. Only modify the parts that are highlighted below to fill in your own values. bioproject : Project_test_localhost experiment : rnaseq_pilot sample_manifest : fastq_file : /users/:bluetext:`username/s ample_manifest_min . csv ` metadata : run_parms : conda_command : source activate /gpfs/runtime/opt/conda/envs/ cbc_conda_test work_dir : /gpfs/scratch/'user/ test_workflow ' log_dir: logs paired_end: False local_targets: False db: sqlite db_loc: \":memory:\" saga_host: localhost ssh_user: ' ccv username ' saga_scheduler : slurm gtf_file : /gpfs/data/cbc/cbcollab/ref_tools/Ensembl_hg_GRCh37_rel87/ Homo_sapiens . GRCh37 . 87 . gtf workflow_sequence : fastqc : default gsnap : options : - d : Ensembl_Homo_sapiens_GRCh37 - s : Ensembl_Homo_sapiens . GRCh37 . 87 . splicesites . iit job_params : ncpus : 16 mem : 40000 time : 60 qualimap_rnaseq : default htseq - count : default How to run Copy the above into a text file and save it in /users/username as test_run.yaml Copy the manifest below into a text file and save it in /users/username as sample_manifest_min.csv samp_1299,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb2_1.gz,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb2_2.gz samp_1214,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb_1.gz,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb_2.gz Now in your screen session run the following commands to setup your environment if you have not done so previously during the setup or you have started a new screen session source activate bflows bioflows-rnaseq test_run.yaml In this case I have created a small test dataset with 10000 reads from a test human RNAseq data, so it should run within the hour and you should see that the alignments are completed.","title":"Specifications"},{"location":"yaml_description/#bioflows-yaml-control-file-specifications","text":"","title":"bioflows YAML control file Specifications"},{"location":"yaml_description/#project-information-section","text":"bioproject : This will be an unique identifier for your project. This is adopted from the NCBI SRA format structure, so if you use an SRA dataset you can employ these ids experiment : An identifier for your experiment such as RNA-seq, ChIP-seq etc sample_manifest : This section contains two sections fastq_file : The full path to the sample to fastq map file. This file is in a three column comma separated format with each line formatted as: sample_id, path_to_fastq_file_for_read1, path_to_fastq_file_for_read2 The sample_id is unique and if you are using single end data you just need to specify one column as shown below: sample_id, path_to_fastq_file metadata : This is all the metadata associated with a given sample_id if available such as gender, extraction date etc. This should also be a CSV format file. Currently, not necessary as this information is not yet used","title":"Project information section"},{"location":"yaml_description/#global-run-parameters","text":"run_parms : This section specifies the global parameters for the current analysis conda_command : This is the command used to activate your conda environment work_dir : The working directory for analysis usually created on /gpfs/scratch log_dir : The subdirectory for all the log files paired_end : Whether data consists of paired end reads or single end reads (True/False) Warning This is an experimental feature and may not work as intended ssh_user : The user name if workflow is run from a local machine saga_scheduler : The scheduler being used, for CCV the value used here is slurm. !!! note Currently only tested with slurm scheduler. Will add test to others soon gtf_file : The full path to the gtf file for gene annotations, needed if you are planning to run rna-seq analysis","title":"Global run parameters"},{"location":"yaml_description/#workflow-parameters","text":"workflow_sequence : This section specifies the sequence of tools to be used and the options passed to tools as well as the job parameters if using a scheduler such as slurm fastqc :If you want to use the default parameters use default else you can use any of the options provided by the program. See the example for GSNAP below on how to do that. See the documentation for the options for fastqc. gsnap : Here we give an example of two sections as we need to pass the index information to the aligner options : Specify program options here. In this example we specify the following -d: The genome index for GSNAP -s: and the splicesites file location for GSNAP. The format is exactly that as to what you would specify on the command line for the program -d Ensembl_mus_GRCm38.p5_rel89 -s Mus_musculus.GRCm38.89.splicesites.iit See the documentation for the GSNAP program for more options job_param s: This section specifies parameters for job submission such as memory, number of cores etc ncpus: 16 mem: 40000 time: 60 qualimap_rnaseq : Run the qualimap module for RNAseq with the default settings The final YAML control file should look as below to run a test example. Only modify the parts that are highlighted below to fill in your own values. bioproject : Project_test_localhost experiment : rnaseq_pilot sample_manifest : fastq_file : /users/:bluetext:`username/s ample_manifest_min . csv ` metadata : run_parms : conda_command : source activate /gpfs/runtime/opt/conda/envs/ cbc_conda_test work_dir : /gpfs/scratch/'user/ test_workflow ' log_dir: logs paired_end: False local_targets: False db: sqlite db_loc: \":memory:\" saga_host: localhost ssh_user: ' ccv username ' saga_scheduler : slurm gtf_file : /gpfs/data/cbc/cbcollab/ref_tools/Ensembl_hg_GRCh37_rel87/ Homo_sapiens . GRCh37 . 87 . gtf workflow_sequence : fastqc : default gsnap : options : - d : Ensembl_Homo_sapiens_GRCh37 - s : Ensembl_Homo_sapiens . GRCh37 . 87 . splicesites . iit job_params : ncpus : 16 mem : 40000 time : 60 qualimap_rnaseq : default htseq - count : default","title":"Workflow parameters"},{"location":"yaml_description/#how-to-run","text":"Copy the above into a text file and save it in /users/username as test_run.yaml Copy the manifest below into a text file and save it in /users/username as sample_manifest_min.csv samp_1299,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb2_1.gz,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb2_2.gz samp_1214,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb_1.gz,/gpfs/scratch/aragaven/rnaseq_test/PE_hg/Cb_2.gz Now in your screen session run the following commands to setup your environment if you have not done so previously during the setup or you have started a new screen session source activate bflows bioflows-rnaseq test_run.yaml In this case I have created a small test dataset with 10000 reads from a test human RNAseq data, so it should run within the hour and you should see that the alignments are completed.","title":"How to run"},{"location":"tutorials/Setup_bioflows_env/","text":"Setting up the environment for bioflows This section is specifically aimed at users of Brown University's OSCAR HPC cluster. Users of other systems should pay close attention to the file paths used, as they may be different for their particular system. Setup conda environment Setup on OSCAR HPC Cluster at Brown University First load the cbc_conda environment as follows: source /gpfs/runtime/cbc_conda/activate_cbc_conda Note Sometimes it may appear that there is no response. In that case make sure to wait for 2-3 minutes and the use cntrl-c to interrupt. You should see something like the below in your command prompt (cbc_conda_v1) [your_ccv_username@login004 mydir] run the echo $PATH command to confirm that /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin is the first element in the list of paths in your output. Error in Paths If /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin is not the first element in the list, then use the command export PATH=/gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin:$PATH This will add /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin to the beginning of your PATH variable For convenience we will use /users/username as the working directory. You should modify this parameter to reflect the path to your own preferred working directory. Setup on other systems Install conda from anaconda following their installation instruction Setup GNU screen session These scripts should be run in a persistent terminal session and we will use GNU screen to do that, which will allow us to disconnnect from our ssh sessions without disrupting long running jobs. To learn more on how to use screen use the following link gnu screen tutorial Change into your working directiory and start a screen session naming it test_bioflows as shown below: cd /users/username screen -S test_bioflows Once you are in your screen session, set up your conda environment containing bioflows source /gpfs/runtime/cbc_conda/activate_cbc_conda Activating the conda environment may take a few moments, but you should see a prompt that looks like (cbc_conda_v1) [your_ccv_username@login004 ~]$ If you don't see the prompt, you may need to press enter again (or cntrl-c )to get your terminal window to refresh. Now you are ready to run the predefined workflows in the tutorials.","title":"Setup the bioflows environment"},{"location":"tutorials/Setup_bioflows_env/#setting-up-the-environment-for-bioflows","text":"This section is specifically aimed at users of Brown University's OSCAR HPC cluster. Users of other systems should pay close attention to the file paths used, as they may be different for their particular system.","title":"Setting up the environment for bioflows"},{"location":"tutorials/Setup_bioflows_env/#setup-conda-environment","text":"","title":"Setup conda environment"},{"location":"tutorials/Setup_bioflows_env/#setup-on-oscar-hpc-cluster-at-brown-university","text":"First load the cbc_conda environment as follows: source /gpfs/runtime/cbc_conda/activate_cbc_conda Note Sometimes it may appear that there is no response. In that case make sure to wait for 2-3 minutes and the use cntrl-c to interrupt. You should see something like the below in your command prompt (cbc_conda_v1) [your_ccv_username@login004 mydir] run the echo $PATH command to confirm that /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin is the first element in the list of paths in your output. Error in Paths If /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin is not the first element in the list, then use the command export PATH=/gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin:$PATH This will add /gpfs/runtime/cbc_conda/cbc_conda_v1_root/envs/cbc_conda_v1/bin to the beginning of your PATH variable For convenience we will use /users/username as the working directory. You should modify this parameter to reflect the path to your own preferred working directory.","title":"Setup on OSCAR HPC Cluster at Brown University"},{"location":"tutorials/Setup_bioflows_env/#setup-on-other-systems","text":"Install conda from anaconda following their installation instruction","title":"Setup on other systems"},{"location":"tutorials/Setup_bioflows_env/#setup-gnu-screen-session","text":"These scripts should be run in a persistent terminal session and we will use GNU screen to do that, which will allow us to disconnnect from our ssh sessions without disrupting long running jobs. To learn more on how to use screen use the following link gnu screen tutorial Change into your working directiory and start a screen session naming it test_bioflows as shown below: cd /users/username screen -S test_bioflows Once you are in your screen session, set up your conda environment containing bioflows source /gpfs/runtime/cbc_conda/activate_cbc_conda Activating the conda environment may take a few moments, but you should see a prompt that looks like (cbc_conda_v1) [your_ccv_username@login004 ~]$ If you don't see the prompt, you may need to press enter again (or cntrl-c )to get your terminal window to refresh. Now you are ready to run the predefined workflows in the tutorials.","title":"Setup GNU screen session"},{"location":"tutorials/qiime2_tutorial/","text":"QIIME2 Overview This tutorial shows how to run a standard predefined QIIME2 analysis on the Brown HPC cluster OSCAR, using the bioflows tool. The particular analysis is the first half of the Moving pictures tutorial from QIIME2. We will assume that you have run through the RNA-Seq tutorial and know how to set up a control file, create a working directory, and setup a screen session as well as have the prerequisites set up. The following is more details specific to the workflow and YAML setup. Getting Started The workflow consists of the following steps: qiime tools import for importing raw amplicon sequencing data into a QIIME2 artifact qiime demux for demultiplexing data qiime dada2 for detecting and correcting data and creating feature tables and representative sequences qiime feature-table for summarizing and visualizing the feature table and representative sequences qiime phylogeny align-to-tree-mafft-fasttree for multiple sequence alignment and phylogeny inference (with mafft and fasttree) qiime diversity core-metrics-phylogenetic for computing alpha and beta diversity statistics qiime feature-classifier classify-sklearn for taxonomic assignment qiime taxa barplot for generating interactive taxonomy barplots qiime composition for differential abundance testing with ANCOM Setup the YAML configuration file (control file) For the current example, copy the following code into a text file and save it in /users/username as test_run.yaml . Note Don't forget to edit the work_dir parameter to reflect the path to your own working directory. bioproject: Project_test_localhost # Project Name Required experiment: qiime_pilot # Experiment type Required sample_manifest: qiime: --type: EMPSingleEndSequences --input-path: emp-single-end-sequences --output-path: emp-single-end-sequences.qza --m-barcodes-file: sample-metadata.tsv #--output-suffix: test1 run_parms: conda_command: source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda; conda activate qiime2-2019.1 work_dir: */users/username* log_dir: logs paired_end: True local_targets: False saga_host: localhost ssh_user: *ccv username* saga_scheduler: slurm reference_fasta_path: /gpfs/scratch/test.fa gtf_file: /gpfs/scratch/aragaven/lapierre/caenorhabditis_elegans.PRJNA13758.WBPS8.canonical_geneset.gtf workflow_sequence: - qiime: subcommand: \"demux emp-single\" options: --i-seqs: emp-single-end-sequences.qza --m-barcodes-file: sample-metadata.tsv --m-barcodes-column: BarcodeSequence --o-per-sample-sequences: demux.qza - qiime: subcommand: demux summarize options: --i-data: demux.qza --o-visualization: demux.qzv - qiime: subcommand: \"dada2 denoise-single\" options: --i-demultiplexed-seqs: demux.qza --p-trim-left: 0 --p-trunc-len: 120 --o-representative-sequences: rep-seqs-dada2.qza --o-table: table-dada2.qza --o-denoising-stats: stats-dada2.qza - qiime: subcommand: metadata tabulate options: --m-input-file: stats-dada2.qza --o-visualization: stats-dada2.qzv - qiime: subcommand: feature-table summarize options: --i-table: table.qza --o-visualization: table.qzv --m-sample-metadata-file: sample-metadata.tsv - qiime: subcommands: feature-table tabulate-seqs options: --i-data rep-seqs.qza --o-visualization rep-seqs.qzv - qiime: subcommand: phylogeny align-to-tree-mafft-fasttree options: --p-n-threads: 2 --i-sequences: rep-seqs.qza --o-alignment: aligned-rep-seqs.qza --o-masked-alignment: masked-aligned-rep-seqs.qza --o-tree: unrooted-tree.qza --o-rooted-tree: rooted-tree.qza Submit the workflow If you haven't done so already, copy the above into a text file and save it in /users/username as test_run.yaml . The data here is the same as from the Moving pictures tutorial. Because it follows the EMP format, no manifest file is needed, but if providing other data the user will need to provide a manifest file matching the description in QIIME2 , specified in the YAML in the same way as in the RNA-seq tutorial If you haven't already started a screen session in the setup , start one using the following command: screen -S rnaseq_tutorial In your screen session, run the following commands to setup your conda environment (if you have not done so previously during the setup or if you just started a new screen session). source activate_cbc_conda bioflows-qiime2 test_run.yaml (TODO: bioflows-qiime2 is not a defined wrapper...) Workflow outputs The bioflows-qiime2 call will automatically generate several directories, which may or may not have any outputs directed to them depending on which analyses have been run in bioflows. These directories include: qiime2 , slurm_scripts , logs , and checkpoints . (TODO: not actually sure what output gets made) sra Will be empty in this tutorial. fastq symlinks to fastq files. alignments SAM and BAM files from GSNAP alignments. qc QC reports from fastqc and qualimap. slurm_scripts Records of the commands sent to slurm. logs Log files from various bioflows processes (including the standard error and standard out). expression Expression values from featureCounts. checkpoints Contains checkpoint records to confirm that bioflows has progressed through each step of the analysis.","title":"Qiime2 tutorial"},{"location":"tutorials/qiime2_tutorial/#qiime2","text":"","title":"QIIME2"},{"location":"tutorials/qiime2_tutorial/#overview","text":"This tutorial shows how to run a standard predefined QIIME2 analysis on the Brown HPC cluster OSCAR, using the bioflows tool. The particular analysis is the first half of the Moving pictures tutorial from QIIME2. We will assume that you have run through the RNA-Seq tutorial and know how to set up a control file, create a working directory, and setup a screen session as well as have the prerequisites set up. The following is more details specific to the workflow and YAML setup.","title":"Overview"},{"location":"tutorials/qiime2_tutorial/#getting-started","text":"","title":"Getting Started"},{"location":"tutorials/qiime2_tutorial/#the-workflow-consists-of-the-following-steps","text":"qiime tools import for importing raw amplicon sequencing data into a QIIME2 artifact qiime demux for demultiplexing data qiime dada2 for detecting and correcting data and creating feature tables and representative sequences qiime feature-table for summarizing and visualizing the feature table and representative sequences qiime phylogeny align-to-tree-mafft-fasttree for multiple sequence alignment and phylogeny inference (with mafft and fasttree) qiime diversity core-metrics-phylogenetic for computing alpha and beta diversity statistics qiime feature-classifier classify-sklearn for taxonomic assignment qiime taxa barplot for generating interactive taxonomy barplots qiime composition for differential abundance testing with ANCOM","title":"The workflow consists of the following steps:"},{"location":"tutorials/qiime2_tutorial/#setup-the-yaml-configuration-file-control-file","text":"For the current example, copy the following code into a text file and save it in /users/username as test_run.yaml . Note Don't forget to edit the work_dir parameter to reflect the path to your own working directory. bioproject: Project_test_localhost # Project Name Required experiment: qiime_pilot # Experiment type Required sample_manifest: qiime: --type: EMPSingleEndSequences --input-path: emp-single-end-sequences --output-path: emp-single-end-sequences.qza --m-barcodes-file: sample-metadata.tsv #--output-suffix: test1 run_parms: conda_command: source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda; conda activate qiime2-2019.1 work_dir: */users/username* log_dir: logs paired_end: True local_targets: False saga_host: localhost ssh_user: *ccv username* saga_scheduler: slurm reference_fasta_path: /gpfs/scratch/test.fa gtf_file: /gpfs/scratch/aragaven/lapierre/caenorhabditis_elegans.PRJNA13758.WBPS8.canonical_geneset.gtf workflow_sequence: - qiime: subcommand: \"demux emp-single\" options: --i-seqs: emp-single-end-sequences.qza --m-barcodes-file: sample-metadata.tsv --m-barcodes-column: BarcodeSequence --o-per-sample-sequences: demux.qza - qiime: subcommand: demux summarize options: --i-data: demux.qza --o-visualization: demux.qzv - qiime: subcommand: \"dada2 denoise-single\" options: --i-demultiplexed-seqs: demux.qza --p-trim-left: 0 --p-trunc-len: 120 --o-representative-sequences: rep-seqs-dada2.qza --o-table: table-dada2.qza --o-denoising-stats: stats-dada2.qza - qiime: subcommand: metadata tabulate options: --m-input-file: stats-dada2.qza --o-visualization: stats-dada2.qzv - qiime: subcommand: feature-table summarize options: --i-table: table.qza --o-visualization: table.qzv --m-sample-metadata-file: sample-metadata.tsv - qiime: subcommands: feature-table tabulate-seqs options: --i-data rep-seqs.qza --o-visualization rep-seqs.qzv - qiime: subcommand: phylogeny align-to-tree-mafft-fasttree options: --p-n-threads: 2 --i-sequences: rep-seqs.qza --o-alignment: aligned-rep-seqs.qza --o-masked-alignment: masked-aligned-rep-seqs.qza --o-tree: unrooted-tree.qza --o-rooted-tree: rooted-tree.qza","title":"Setup the YAML configuration file (control file)"},{"location":"tutorials/qiime2_tutorial/#submit-the-workflow","text":"If you haven't done so already, copy the above into a text file and save it in /users/username as test_run.yaml . The data here is the same as from the Moving pictures tutorial. Because it follows the EMP format, no manifest file is needed, but if providing other data the user will need to provide a manifest file matching the description in QIIME2 , specified in the YAML in the same way as in the RNA-seq tutorial If you haven't already started a screen session in the setup , start one using the following command: screen -S rnaseq_tutorial In your screen session, run the following commands to setup your conda environment (if you have not done so previously during the setup or if you just started a new screen session). source activate_cbc_conda bioflows-qiime2 test_run.yaml (TODO: bioflows-qiime2 is not a defined wrapper...)","title":"Submit the workflow"},{"location":"tutorials/qiime2_tutorial/#workflow-outputs","text":"The bioflows-qiime2 call will automatically generate several directories, which may or may not have any outputs directed to them depending on which analyses have been run in bioflows. These directories include: qiime2 , slurm_scripts , logs , and checkpoints . (TODO: not actually sure what output gets made) sra Will be empty in this tutorial. fastq symlinks to fastq files. alignments SAM and BAM files from GSNAP alignments. qc QC reports from fastqc and qualimap. slurm_scripts Records of the commands sent to slurm. logs Log files from various bioflows processes (including the standard error and standard out). expression Expression values from featureCounts. checkpoints Contains checkpoint records to confirm that bioflows has progressed through each step of the analysis.","title":"Workflow outputs"},{"location":"tutorials/rna-seq_tutorial/","text":"RNAseq with GSNAP Overview This tutorial shows how to run a standard predefined RNA-seq analysis on the Brown HPC cluster OSCAR, using the bioflows tool. A visual overview of the workflow is shown below Getting Started Basic workflow The workflow consists of the following programs run sequentially on each sample: Fastqc : For QC of Raw Fastq reads Trimmomatic : Quality and Adapter trimming of raw reads Fastqc : Post trimming QC of reads GSNAP : alignment of the reads to the reference genome Samtools : To process alignments: convert sam to bam remove unmapped reads coordinate sort bam index bam biobambam Suite bamsort Sort bams bammarkduplicates2 Mark duplicates Qualimap QC of the aligments generated featureCounts/HTseq Expression quantification based on counting mapped reads Salmon : Alignment free quantification of known transcripts Depending on the user's need the workflow can be adapted for just using a subset of steps or the changes in the sequence as long as they are sensible. We povide a couple of alternative workflows using combinations of the steps in the basic workflow Steps The basic steps to running a workflow are: Create a control file Create your working directory if does not exist, here we assume it is /users/mydir . Setup a screen session The next section provide a short how-to with all the commands to execute the test workflow on Brown University's OSCAR HPCcluster. Once you have the test case working you can implement this on your own data Prerequisites Make sure you have access to the OSCAR cluster or request one by contacting support@ccv.brown.edu. If you are not comfortable with the Linux environment, you can consult the tutorial here. You should also set up bioflows. Danger You need to have an priority account on OSCAR to run this with real datasets as the resources for exploratory accounts are not sufficient. Caution The working directory in a real example can end up being quite large: up to a few terabytes. On OSCAR, you would create the working directory in a location such as your data folder or the scratch folder. Running the workflow Create the YAML file Bioflows uses YAML configuration files to run workflows. A detailed documentation of the YAML file and all the options is shown here . For the current example, copy the following code into a text file and save it in /users/mydir as test_run.yaml . Note Edit the parameters in the highlighted lines to change values specific to your username bioproject : Project_test_localhost experiment : rnaseq_pilot sample_manifest : fastq_file : sample_manifest_min.csv metadata : run_parms : conda_command : source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda work_dir : /users/mydir log_dir : logs paired_end : True local_targets : False saga_host : localhost ssh_user : ccv username saga_scheduler : slurm gtf_file : /gpfs/data/cbc/cbcollab/ref_tools/Ensembl_hg_GRCh37_rel87/Homo_sapiens.GRCh37.87.gtf workflow_sequence : - fastqc : default - gsnap : options : -d : Ensembl_Homo_sapiens_GRCh37 -s : /gpfs/data/cbc/cbcollab/cbc_ref/gmapdb_2017.01.14/Ensembl_Homo_sapiens_GRCh37/Ensembl_Homo_sapiens_GRCh37.maps/Ensembl_Homo_sapiens.GRCh37.87.splicesites.iit job_params : ncpus : 8 mem : 40000 time : 60 - samtools : subcommand : view suffix : input : \".sam\" output : \".bam\" options : -Sbh : job_params : time : 60 - samtools : subcommand : view suffix : input : \".bam\" output : \".mapped.bam\" options : -bh : -F : \"0x4\" job_params : time : 60 - samtools : subcommand : view suffix : input : \".bam\" output : \".unmapped.bam\" options : -f : \"0x4\" - bamsort : suffix : input : \".mapped.bam\" output : \".srtd.bam\" options : inputthreads=4 : outputthreads=4 : job_params : ncpus : 4 mem : 2000 time : 60 - samtools : subcommand : index suffix : input : \".srtd.bam\" job_params : time : 20 - bammarkduplicates2 : suffix : input : \".srtd.bam\" output : \".dup.srtd.bam\" job_params : mem : 2000 time : 60 ncpus : 4 - samtools : subcommand : index suffix : input : \".dup.srtd.bam\" job_params : time : 20 - qualimap : subcommand : rnaseq - htseq-count : default - featureCounts : default If you haven't done so already, copy the above into a text file and save it in /users/mydir as test_run.yaml For this tutorial I have created a small test dataset with 10000 read pairs from human RNAseq data, that's available to all users on OSCAR. It should run within the hour and you should see that all the steps from the workflow have completed. Create the manifest file We will now create the sample manifest file, which is in csv format. You can find more information about sample manifest files here . Copy the manifest below into a text file and save it in /users/mydir as sample_manifest_min.csv . As you might notice this is a comma-seperated file. samp_1299,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb2_1.gz,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb2_2.gz samp_1214,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb_1.gz,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb_2.gz Run the workflow in a screen session If you haven't already started a screen session in the setup , start one using the following command: screen -S rnaseq_tutorial In your screen session, run the following commands to setup your conda environment (if you have not done so previously during the setup or if you just started a new screen session). source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda bioflows-run test_run.yaml You can now exit the screen session or wait till you get the smiley face that provides completion Workflow outputs The bioflows-run call will automatically generate several directories, which may or may not have any outputs directed to them depending on which analyses have been run in bioflows. These directories include: sra , fastq , alignments , qc , slurm_scripts , logs , expression , and checkpoints . For this tutorial you should see the following directories fastq symlinks to fastq files. alignments SAM and BAM files from GSNAP alignments. qc QC reports from fastqc and qualimap. slurm_scripts Records of the commands sent to slurm. logs Log files from various bioflows processes (including the standard error and standard out). expression Expression values from featureCounts/Salmon/htseq depending on whats specified. checkpoints Contains checkpoint records to confirm that bioflows has progressed through each step of the analysis. Within each folder you will see files that are annotated by samplename_program_suffix . We provide a list of outputs and their contents below Alternative workflow YAMLs These YAML's can be used as templates for alternative workflows using various combinations of programs and sequences from the programs defined in the Basic workflow","title":"RNA-Seq tutorial"},{"location":"tutorials/rna-seq_tutorial/#rnaseq-with-gsnap","text":"","title":"RNAseq with GSNAP"},{"location":"tutorials/rna-seq_tutorial/#overview","text":"This tutorial shows how to run a standard predefined RNA-seq analysis on the Brown HPC cluster OSCAR, using the bioflows tool. A visual overview of the workflow is shown below","title":"Overview"},{"location":"tutorials/rna-seq_tutorial/#getting-started","text":"","title":"Getting Started"},{"location":"tutorials/rna-seq_tutorial/#basic-workflow","text":"The workflow consists of the following programs run sequentially on each sample: Fastqc : For QC of Raw Fastq reads Trimmomatic : Quality and Adapter trimming of raw reads Fastqc : Post trimming QC of reads GSNAP : alignment of the reads to the reference genome Samtools : To process alignments: convert sam to bam remove unmapped reads coordinate sort bam index bam biobambam Suite bamsort Sort bams bammarkduplicates2 Mark duplicates Qualimap QC of the aligments generated featureCounts/HTseq Expression quantification based on counting mapped reads Salmon : Alignment free quantification of known transcripts Depending on the user's need the workflow can be adapted for just using a subset of steps or the changes in the sequence as long as they are sensible. We povide a couple of alternative workflows using combinations of the steps in the basic workflow","title":"Basic workflow"},{"location":"tutorials/rna-seq_tutorial/#steps","text":"The basic steps to running a workflow are: Create a control file Create your working directory if does not exist, here we assume it is /users/mydir . Setup a screen session The next section provide a short how-to with all the commands to execute the test workflow on Brown University's OSCAR HPCcluster. Once you have the test case working you can implement this on your own data Prerequisites Make sure you have access to the OSCAR cluster or request one by contacting support@ccv.brown.edu. If you are not comfortable with the Linux environment, you can consult the tutorial here. You should also set up bioflows. Danger You need to have an priority account on OSCAR to run this with real datasets as the resources for exploratory accounts are not sufficient. Caution The working directory in a real example can end up being quite large: up to a few terabytes. On OSCAR, you would create the working directory in a location such as your data folder or the scratch folder.","title":"Steps"},{"location":"tutorials/rna-seq_tutorial/#running-the-workflow","text":"","title":"Running the workflow"},{"location":"tutorials/rna-seq_tutorial/#create-the-yaml-file","text":"Bioflows uses YAML configuration files to run workflows. A detailed documentation of the YAML file and all the options is shown here . For the current example, copy the following code into a text file and save it in /users/mydir as test_run.yaml . Note Edit the parameters in the highlighted lines to change values specific to your username bioproject : Project_test_localhost experiment : rnaseq_pilot sample_manifest : fastq_file : sample_manifest_min.csv metadata : run_parms : conda_command : source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda work_dir : /users/mydir log_dir : logs paired_end : True local_targets : False saga_host : localhost ssh_user : ccv username saga_scheduler : slurm gtf_file : /gpfs/data/cbc/cbcollab/ref_tools/Ensembl_hg_GRCh37_rel87/Homo_sapiens.GRCh37.87.gtf workflow_sequence : - fastqc : default - gsnap : options : -d : Ensembl_Homo_sapiens_GRCh37 -s : /gpfs/data/cbc/cbcollab/cbc_ref/gmapdb_2017.01.14/Ensembl_Homo_sapiens_GRCh37/Ensembl_Homo_sapiens_GRCh37.maps/Ensembl_Homo_sapiens.GRCh37.87.splicesites.iit job_params : ncpus : 8 mem : 40000 time : 60 - samtools : subcommand : view suffix : input : \".sam\" output : \".bam\" options : -Sbh : job_params : time : 60 - samtools : subcommand : view suffix : input : \".bam\" output : \".mapped.bam\" options : -bh : -F : \"0x4\" job_params : time : 60 - samtools : subcommand : view suffix : input : \".bam\" output : \".unmapped.bam\" options : -f : \"0x4\" - bamsort : suffix : input : \".mapped.bam\" output : \".srtd.bam\" options : inputthreads=4 : outputthreads=4 : job_params : ncpus : 4 mem : 2000 time : 60 - samtools : subcommand : index suffix : input : \".srtd.bam\" job_params : time : 20 - bammarkduplicates2 : suffix : input : \".srtd.bam\" output : \".dup.srtd.bam\" job_params : mem : 2000 time : 60 ncpus : 4 - samtools : subcommand : index suffix : input : \".dup.srtd.bam\" job_params : time : 20 - qualimap : subcommand : rnaseq - htseq-count : default - featureCounts : default If you haven't done so already, copy the above into a text file and save it in /users/mydir as test_run.yaml For this tutorial I have created a small test dataset with 10000 read pairs from human RNAseq data, that's available to all users on OSCAR. It should run within the hour and you should see that all the steps from the workflow have completed.","title":"Create the YAML file"},{"location":"tutorials/rna-seq_tutorial/#create-the-manifest-file","text":"We will now create the sample manifest file, which is in csv format. You can find more information about sample manifest files here . Copy the manifest below into a text file and save it in /users/mydir as sample_manifest_min.csv . As you might notice this is a comma-seperated file. samp_1299,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb2_1.gz,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb2_2.gz samp_1214,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb_1.gz,/gpfs/data/cbc/rnaseq_test_data/PE_hg/Cb_2.gz","title":"Create the manifest file"},{"location":"tutorials/rna-seq_tutorial/#run-the-workflow-in-a-screen-session","text":"If you haven't already started a screen session in the setup , start one using the following command: screen -S rnaseq_tutorial In your screen session, run the following commands to setup your conda environment (if you have not done so previously during the setup or if you just started a new screen session). source /gpfs/runtime/cbc_conda/bin/activate_cbc_conda bioflows-run test_run.yaml You can now exit the screen session or wait till you get the smiley face that provides completion","title":"Run the workflow in a screen session"},{"location":"tutorials/rna-seq_tutorial/#workflow-outputs","text":"The bioflows-run call will automatically generate several directories, which may or may not have any outputs directed to them depending on which analyses have been run in bioflows. These directories include: sra , fastq , alignments , qc , slurm_scripts , logs , expression , and checkpoints . For this tutorial you should see the following directories fastq symlinks to fastq files. alignments SAM and BAM files from GSNAP alignments. qc QC reports from fastqc and qualimap. slurm_scripts Records of the commands sent to slurm. logs Log files from various bioflows processes (including the standard error and standard out). expression Expression values from featureCounts/Salmon/htseq depending on whats specified. checkpoints Contains checkpoint records to confirm that bioflows has progressed through each step of the analysis. Within each folder you will see files that are annotated by samplename_program_suffix . We provide a list of outputs and their contents below","title":"Workflow outputs"},{"location":"tutorials/rna-seq_tutorial/#alternative-workflow-yamls","text":"These YAML's can be used as templates for alternative workflows using various combinations of programs and sequences from the programs defined in the Basic workflow","title":"Alternative workflow YAMLs"},{"location":"tutorials/tutorials/","text":"About bioflows Tutorials This section provides an overview of how to run pre-defined workflows using the bioflows package. The tutorials are based on the presumption that the analysis is conducted on the Brown OSCAR compute cluster To use on other systems please check the installation instructions for setting up your environment appropriately. Currently, we provide tutorials for implementing an RNA-seq workflow using the GSNAP RNAseq aligner, Variant Calling with GATK and 16S sequence analysis with QIIME2. This will be updated as new workflows and enhancements are made to the bioflows package. Note If you are not sure what the console means or how to login to OSCAR go here .","title":"About"},{"location":"tutorials/tutorials/#about-bioflows-tutorials","text":"This section provides an overview of how to run pre-defined workflows using the bioflows package. The tutorials are based on the presumption that the analysis is conducted on the Brown OSCAR compute cluster To use on other systems please check the installation instructions for setting up your environment appropriately. Currently, we provide tutorials for implementing an RNA-seq workflow using the GSNAP RNAseq aligner, Variant Calling with GATK and 16S sequence analysis with QIIME2. This will be updated as new workflows and enhancements are made to the bioflows package. Note If you are not sure what the console means or how to login to OSCAR go here .","title":"About bioflows Tutorials"}]}